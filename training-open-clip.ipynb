{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94469ff-6d95-4ca1-9eff-9bac37233953",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 53718,
     "status": "ok",
     "timestamp": 1698834665580,
     "user": {
      "displayName": "marco amerotti",
      "userId": "18074794572726120264"
     },
     "user_tz": -60
    },
    "id": "c94469ff-6d95-4ca1-9eff-9bac37233953",
    "outputId": "e8155d9a-8b34-4f85-8d2c-4c512596998b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install open_clip_torch\n",
    "# !pip install transformers\n",
    "# !pip install torch # DO NOT RUN IN GCP NOTEBOOK!!!\n",
    "# !pip install open_clip_torch\n",
    "# !pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409292ae-6685-4e0c-8e2a-b9f7665ebe10",
   "metadata": {
    "executionInfo": {
     "elapsed": 13518,
     "status": "ok",
     "timestamp": 1698836333053,
     "user": {
      "displayName": "marco amerotti",
      "userId": "18074794572726120264"
     },
     "user_tz": -60
    },
    "id": "409292ae-6685-4e0c-8e2a-b9f7665ebe10"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import open_clip\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X-Pm6stAcJHL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 2511,
     "status": "ok",
     "timestamp": 1698836337046,
     "user": {
      "displayName": "marco amerotti",
      "userId": "18074794572726120264"
     },
     "user_tz": -60
    },
    "id": "X-Pm6stAcJHL",
    "outputId": "68dd3575-865f-4a89-f5dc-0e6b40d77cd6"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8917410-b80b-4c69-b65e-0e6a57af97b0",
   "metadata": {
    "executionInfo": {
     "elapsed": 1150,
     "status": "ok",
     "timestamp": 1698836338900,
     "user": {
      "displayName": "marco amerotti",
      "userId": "18074794572726120264"
     },
     "user_tz": -60
    },
    "id": "d8917410-b80b-4c69-b65e-0e6a57af97b0"
   },
   "outputs": [],
   "source": [
    "# Loading the training dataset\n",
    "subset = pd.read_csv('train_subset.csv') # Change this if you want to use other data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc0c77d-a26d-4262-8a9e-2621bbdc9381",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1698836340010,
     "user": {
      "displayName": "marco amerotti",
      "userId": "18074794572726120264"
     },
     "user_tz": -60
    },
    "id": "0fc0c77d-a26d-4262-8a9e-2621bbdc9381"
   },
   "outputs": [],
   "source": [
    "def get_image_file_path(id):\n",
    "    folder_id = \"0\" + str(id)\n",
    "    first_three = str(folder_id)[:3]\n",
    "    image_path = f\"images/{first_three}/{folder_id}.jpg\"\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff08b1-877d-4f99-aab5-ef59fd1854bd",
   "metadata": {
    "executionInfo": {
     "elapsed": 937,
     "status": "ok",
     "timestamp": 1698836346087,
     "user": {
      "displayName": "marco amerotti",
      "userId": "18074794572726120264"
     },
     "user_tz": -60
    },
    "id": "95ff08b1-877d-4f99-aab5-ef59fd1854bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dusty light khaki green short dress in a patterned viscose weave with a small stand-up collar and small V-neck opening at the top\n",
      "Medium dusty khaki green top in lightweight sweatshirt fabric in a relaxed fit with long raglan sleeves and ribbing around the neckline, cuffs and hem.\n",
      "Dusty light grey sleeveless top in soft jersey with a sheen\n",
      "Dark mole baby Exclusive\n",
      "Medium blue cold shoulder blouse in woven fabric with narrow, adjustable shoulder straps, elastication and a flounce at the top, long sleeves with elastication at the cuffs, and a smocked hem.\n",
      "Dusty light pink wide sports top in fast-drying functional fabric with a slightly wider neckline, short cap sleeves and a drawstring at one side of the hem\n",
      "Dark black ankle-length jeans in washed, stretch denim with a high waist, zip fly and button, patch front pockets, back pockets and straight, wide legs.\n",
      "Light white shirt in a linen weave in a straight, relaxed style with a grandad collar, classic front and yoke at the back\n",
      "Light white thin tights with a spot pattern and an elasticated waist.\n",
      "Medium dusty blue fully lined, 5-pocket, slim-fit jeans in washed stretch denim with an adjustable elasticated waist (in sizes 8-12Y), zip fly and button, and narrow hems.\n",
      "Dark blue t-shirt in printed cotton jersey.\n",
      " Wide top in sweatshirt fabric with a lined drawstring hood, kangaroo pocket and ribbing at the cuffs and hem.\n",
      "Light blue pyjamas in soft, printed cotton jersey\n",
      "Dark black long-sleeved top in soft cotton jersey with a motif on the front and ribbing around the neckline and cuffs.\n",
      "Dark blue jumper in a soft knit containing glittery threads with a V-neck, dropped shoulders and ribbing around the neckline, cuffs and hem.\n",
      "Bright blue jacquard-knit socks in a soft cotton blend with elastication around the top.\n",
      "Medium dusty grey leggings in soft jersey with a motif and an elasticated waist.\n",
      "Dark grey jacket in sweatshirt fabric with a lined hood, zip down the front, front pockets and ribbing at the cuffs and hem\n",
      "Dark black fitted, knee-length dress in a patterned stretch crêpe weave with cap sleeves and a back-to-front shirt collar with an opening and buttons at the back of the neck\n",
      "Dusty light grey short, sleeveless dress in sturdy jersey with a low-cut back, seam at the waist and flared skirt\n",
      "Dark black jersey cap in a cotton blend with an inner terry sweatband and an adjustable plastic fastener at the back.\n",
      "Medium dusty green jacket in lightweight, patterned sweatshirt fabric with a jersey-lined hood, zip down the front, side pockets and ribbing at the cuffs and hem.\n",
      "Bright blue 5-pocket slim-fit jeans in washed denim with stretch\n",
      "Medium dusty beige trousers in sweatshirt fabric with an elasticated drawstring waist, side pockets, side stripes and ribbed hems.\n",
      "Medium dusty blue scarf in a patterned cotton weave\n",
      "Light blue loose-fitting, sports vest top in soft slub jersey made from a modal and cotton blend\n",
      "Light blue 5-pocket, ankle-length jeans in washed, sturdy cotton denim with a high waist, button fly and slim, straight legs with raw-edge hems.\n",
      "Dusty light blue cardigan in a soft, fine knit with slits in the sides and no buttons.\n",
      "Dark blue imitation leather sandals with an adjustable ankle strap with a hook and loop fastening\n",
      "Medium dusty beige pyjama top and shorts in soft satin\n"
     ]
    }
   ],
   "source": [
    "# concatenate text fields as text input\n",
    "\n",
    "def generate_combined_string(row):\n",
    "    first_sentence = row['detail_desc'].split('. ')[0]\n",
    "    perceived_colour_master_name = row['perceived_colour_master_name'].lower()\n",
    "    if perceived_colour_master_name.lower() in ['undefined', 'unknown']:\n",
    "        colour_string = \"\"\n",
    "    else:\n",
    "        perceived_colour_value_name = row['perceived_colour_value_name']\n",
    "        perceived_colour_value_name = perceived_colour_value_name[0].upper() + perceived_colour_value_name[1:].lower()  # Capitalize first letter, convert rest to lowercase\n",
    "        colour_string = f\"{perceived_colour_value_name} {perceived_colour_master_name}\"\n",
    "        first_sentence = first_sentence[0].lower() + first_sentence[1:]  # Convert first letter to lowercase\n",
    "    return f\"{colour_string} {first_sentence}\"\n",
    "\n",
    "text_data = subset.apply(generate_combined_string, axis=1)\n",
    "\n",
    "# Show some example lines\n",
    "for i in range(30):\n",
    "    print(text_data[i])\n",
    "    \n",
    "image_paths = subset[\"article_id\"].apply(lambda article_id: get_image_file_path(article_id)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094a7d6-6221-4761-bb99-bf48821da6f0",
   "metadata": {
    "executionInfo": {
     "elapsed": 7837,
     "status": "ok",
     "timestamp": 1698836354973,
     "user": {
      "displayName": "marco amerotti",
      "userId": "18074794572726120264"
     },
     "user_tz": -60
    },
    "id": "c094a7d6-6221-4761-bb99-bf48821da6f0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Remove texts which have no corresponding img\n",
    "filtered_texts = list(filter(lambda text_path: os.path.isfile(text_path[1]), zip(text_data, image_paths)))\n",
    "filtered_paths = list(map(lambda text_path: text_path[1], filtered_texts))\n",
    "\n",
    "# Extract the text_data from filtered_texts\n",
    "filtered_texts = list(map(lambda text_path: text_path[0], filtered_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27af8f-b486-46b0-8d33-194b1576fad9",
   "metadata": {
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1698836362474,
     "user": {
      "displayName": "marco amerotti",
      "userId": "18074794572726120264"
     },
     "user_tz": -60
    },
    "id": "8d27af8f-b486-46b0-8d33-194b1576fad9"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 100\n",
    "LR = 1e-7\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7ecf2-ce39-48f5-a753-58449dd68180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:laion/CLIP-ViT-B-32-laion2B-s34B-b79K', device=device)\n",
    "tokenizer = open_clip.get_tokenizer('hf-hub:laion/CLIP-ViT-B-32-laion2B-s34B-b79K')\n",
    "preprocess = preprocess_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438b2706-b205-4025-952c-1da4fcb35823",
   "metadata": {
    "executionInfo": {
     "elapsed": 4082,
     "status": "ok",
     "timestamp": 1698838034734,
     "user": {
      "displayName": "marco amerotti",
      "userId": "18074794572726120264"
     },
     "user_tz": -60
    },
    "id": "438b2706-b205-4025-952c-1da4fcb35823"
   },
   "outputs": [],
   "source": [
    "# Inspiration from https://github.com/openai/CLIP/issues/83\n",
    "\n",
    "class image_title_dataset(Dataset):\n",
    "    def __init__(self, list_image_path,list_txt):\n",
    "\n",
    "        self.image_path = list_image_path\n",
    "        self.title  = tokenizer(list_txt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess(Image.open(self.image_path[idx])) # Image from PIL module\n",
    "        title = self.title[idx]\n",
    "        return image,title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a8cd7-bbb0-4f84-9b12-0ecf906b61a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready the data\n",
    "list_image_path = filtered_paths\n",
    "list_txt = filtered_texts\n",
    "dataset = image_title_dataset(list_image_path,list_txt)\n",
    "\n",
    "# validation set for early stopping\n",
    "train, val = torch.utils.data.random_split(dataset, [0.9, 0.1])\n",
    "train_dataloader = DataLoader(train,batch_size = BATCH_SIZE) #Define your own dataloader\n",
    "val_dataloader = DataLoader(val,batch_size = BATCH_SIZE) #Define your own dataloader\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR,betas=(0.9,0.98),eps=1e-6) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5969b978-7478-4137-8340-87979b202249",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 131921,
     "status": "ok",
     "timestamp": 1698840263716,
     "user": {
      "displayName": "marco amerotti",
      "userId": "18074794572726120264"
     },
     "user_tz": -60
    },
    "id": "5969b978-7478-4137-8340-87979b202249",
    "outputId": "b55778f7-2fb0-410e-92e0-27610ec9bf86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [53:37<00:00,  5.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN EPOCH:\t0,\tLOSS:\t6.227045070713964\n",
      "VAL EPOCH:\t0,\tLOSS:\t6.213827683375432\n",
      "New minimum loss reached at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [53:26<00:00,  5.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN EPOCH:\t1,\tLOSS:\t6.2016163390258265\n",
      "VAL EPOCH:\t1,\tLOSS:\t6.190689783829909\n",
      "New minimum loss reached at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [53:22<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN EPOCH:\t2,\tLOSS:\t6.182627651609224\n",
      "VAL EPOCH:\t2,\tLOSS:\t6.176787618490366\n",
      "New minimum loss reached at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [53:01<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN EPOCH:\t3,\tLOSS:\t6.1726986794636165\n",
      "VAL EPOCH:\t3,\tLOSS:\t6.1703459959763745\n",
      "New minimum loss reached at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [52:52<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN EPOCH:\t4,\tLOSS:\t6.168175218845236\n",
      "VAL EPOCH:\t4,\tLOSS:\t6.167339376302865\n",
      "New minimum loss reached at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [53:01<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN EPOCH:\t5,\tLOSS:\t6.16599769016792\n",
      "VAL EPOCH:\t5,\tLOSS:\t6.16579426251925\n",
      "New minimum loss reached at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [53:15<00:00,  5.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN EPOCH:\t6,\tLOSS:\t6.164820159714798\n",
      "VAL EPOCH:\t6,\tLOSS:\t6.164921797238863\n",
      "New minimum loss reached at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [52:58<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN EPOCH:\t7,\tLOSS:\t6.164113446761822\n",
      "VAL EPOCH:\t7,\tLOSS:\t6.1643769631019\n",
      "New minimum loss reached at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [52:40<00:00,  5.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN EPOCH:\t8,\tLOSS:\t6.163654725304966\n",
      "VAL EPOCH:\t8,\tLOSS:\t6.164038071265588\n",
      "New minimum loss reached at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [52:43<00:00,  5.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN EPOCH:\t9,\tLOSS:\t6.163334690291306\n",
      "VAL EPOCH:\t9,\tLOSS:\t6.163804765848013\n",
      "New minimum loss reached at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 105/580 [09:35<43:37,  5.51s/it]"
     ]
    }
   ],
   "source": [
    "# add your own code to track the training progress.\n",
    "min_loss = 1000000000000\n",
    "min_epoch = None\n",
    "stop_training = False\n",
    "for epoch in range(EPOCH):\n",
    "    train_loss=0.0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images,texts = batch\n",
    "\n",
    "        images= images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        outputs = model(images, texts)\n",
    "        logits_per_image = outputs[0]\n",
    "        logits_per_text = outputs[1]\n",
    "\n",
    "\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "\n",
    "        train_loss+=total_loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    print(f'TRAIN EPOCH:\\t{epoch},\\tLOSS:\\t{train_loss}')\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch in val_dataloader:\n",
    "            images,texts = batch\n",
    "\n",
    "            images= images.to(device)\n",
    "            texts = texts.to(device)\n",
    "\n",
    "            outputs = model(images, texts)\n",
    "            logits_per_image = outputs[0]\n",
    "            logits_per_text = outputs[1]\n",
    "\n",
    "\n",
    "            ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "\n",
    "            total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            val_loss+=total_loss.item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        print(f'VAL EPOCH:\\t{epoch},\\tLOSS:\\t{val_loss}')\n",
    "\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': train_loss, # this total_loss is really just the loss of the most recent sample\n",
    "        }, f\"models/colour_first_lr_{LR}_epoch_{epoch}_valloss_{val_loss}_better.pt\") #just change to your preferred folder/filename\n",
    "\n",
    "    # early stopping\n",
    "    min_loss = min(min_loss, val_loss)\n",
    "    if min_loss == val_loss:\n",
    "        min_epoch = epoch\n",
    "        print(f'New minimum loss reached at epoch {epoch}')\n",
    "\n",
    "    if epoch - min_epoch >= PATIENCE:\n",
    "        stop_training = True\n",
    "        print(f'Loss did not improve in last {PATIENCE} epochs. Stopping.')\n",
    "\n",
    "    if stop_training:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01112cb1-5447-4d7e-860c-bf5bacfe1c5d",
   "metadata": {
    "id": "01112cb1-5447-4d7e-860c-bf5bacfe1c5d"
   },
   "outputs": [],
   "source": [
    "# # To load the fine-tuned model do:\n",
    "# model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:laion/CLIP-ViT-B-32-laion2B-s34B-b79K', device=device)\n",
    "# checkpoint = torch.load('models/coolmodel.pt') # for example models/CLIP-ViT-B-32-laion2B-s34B-b79K_1_epoch.pt\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ade31-ee98-4d96-926c-3b5e6df695b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
